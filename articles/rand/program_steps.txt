How everything fits together
--------------------------------

Compression stage
------------------

1. buf = rand()
2. words = chunks(buf)
3. gcs_table = add_words_to_filter(words)
4. candidate_lists, node_lists = brute force candidate lists(gcs_table)
	- compress()
	- ex_brute_candidates.py -- sample to generate candidates
5. hash_list = compute_edge_hash_list(... 4 pairs of 8 nodes)
	- this is only for one nonce so incomplete
	- do this for all other 'quad sets in node_lists'
	- clusted_shared_pow_manager(... hash_list)
		- ex_cpow_slave.py -- loop code run on one slave
		- ex_cpow_all.py -- loop code to check hash list against cpow
		- ex_cpow_once.py -- check single list item against cpow 
	- ex_create_nonces.py -- one nonce example
6. ex_qset_encode.py -- will select nonces and encode heuristic information. Also has code to reduce size of nonces based on qt_format.





Decompression stage
--------------------


- filter used to extract quad sets
	- ex_qset_decord.py
		- unpack nonces using qt format.
		- brute force search qset using edges
		- use heuristic information and nonces to skip edges so the search space doesn't become intractible 